{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method and Random Forest\n",
    "\n",
    "## 1. Boosting\n",
    "\n",
    "Boosting is ensemble method, by connecting several model which is weak, making more strong model.\n",
    "\n",
    "Basically, It complements previous model. \n",
    "\n",
    "There are two popular boosting model. Adaptive Boosting(AdaBoost) and Gradient Boosting. (I'm going to go over only about Adaboost)\n",
    "\n",
    "Boosting calulates its error(wrong classification) in someway, and then, to fix this, they increase weight on their model.\n",
    "\n",
    "**It is quite similar with gradient descent.**\n",
    "\n",
    "\n",
    "### Adaptive Boosting\n",
    "\n",
    "These two type of model is different in how to complement previous model.\n",
    "\n",
    "Repeat:\n",
    "\n",
    "- First, AdaBoost gives each samples a weight.(1/m).\n",
    "\n",
    "- After training, calculate error and classifier weight.(Different with weight.)\n",
    "\n",
    "- Finally, update weight by multiplying exp(classifier weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.ensemble import AdaBoostClassifier\\n\\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\\n                             algorithm='SAMME.R', learning_rate=0.5)\\n\\nada_clf.fit(X, y)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                             algorithm='SAMME.R', learning_rate=0.5)\n",
    "\n",
    "ada_clf.fit(X, y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is quite unusal algoritm. I'll explain its process step by step.\n",
    "\n",
    "- From dataset, make hold-out set. (Split dataset into two subsets.(sub1 and sub2))\n",
    "\n",
    "- Train several model by sub1.(for example, 3 models)\n",
    "\n",
    "- Draw prediction value of sub2 using trained model, You can get 3-dimensional value.\n",
    "\n",
    "- Use this prediction as input X, and original target value as y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite strong algorithm to improve model performance. Many of kaggle winner used **Stacking**. \n",
    "\n",
    "For example, Winner in Otto Group challnenge used stacking ensemble of over 30 models whose output \n",
    "\n",
    "was used as features for three meta-classifier: XGBoost, Neural Network, and AdaBoost.\n",
    "\n",
    "**30 to 3**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
